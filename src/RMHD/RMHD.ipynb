{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Custom Activation Layers\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from activationLayers import *\n",
    "\n",
    "# Import RMHD Equations\n",
    "from RMHDEquations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Neural Network\n",
    "\n",
    "class RMHDPINN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, n_hidden, hidden_width, activation=TrainableTanh(), output_projection=FinalActivation()):\n",
    "        super(RMHDPINN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_size\n",
    "        self.output_dim = output_size\n",
    "        self.hidden_dim = hidden_width\n",
    "        self.num_layers = n_hidden\n",
    "        \n",
    "        # Define U and V layers separately\n",
    "        self.U_layer = nn.Linear(input_size, hidden_width)\n",
    "        self.V_layer = nn.Linear(input_size, hidden_width)\n",
    "        \n",
    "        # Define hidden and output layers\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_width, hidden_width) for _ in range(n_hidden)])\n",
    "        self.output_layer = nn.Linear(hidden_width, output_size)\n",
    "        \n",
    "        # Set activation function\n",
    "        self.phi = activation\n",
    "        self.output_projection = output_projection\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            t (torch.Tensor): The time argument.\n",
    "            x (torch.Tensor): The position argument.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The primitives for the 1-dimensional RMHD equations.\n",
    "\n",
    "        \"\"\"\n",
    "        t = t.unsqueeze(1) if len(t.shape) == 1 else t\n",
    "        x = x.unsqueeze(1) if len(x.shape) == 1 else x\n",
    "        X = torch.cat([x, t], dim=1)\n",
    "        U_t = self.phi(self.U_layer(X))\n",
    "        V_t = self.phi(self.V_layer(X))\n",
    "        H = U_t  # initial value for H\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            Z = self.phi(layer(H))\n",
    "            H = (1 - Z) * U_t + Z * V_t\n",
    "\n",
    "        output = self.output_layer(H)\n",
    "        # Print the shape of the tensor before it's passed to the FinalActivation\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialization\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMHDPINN(\n",
      "  (U_layer): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (V_layer): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=5, bias=True)\n",
      "  (phi): TrainableTanh()\n",
      "  (output_projection): FinalActivation()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1613, -0.5243,  0.7675, -0.2049, -0.1905],\n",
       "        [-0.0306, -0.6939,  0.4499, -0.0265, -0.1600],\n",
       "        [ 0.1456, -0.5776,  0.7330, -0.1305, -0.2178],\n",
       "        [ 0.1148, -0.5707,  0.6902, -0.1809, -0.1788],\n",
       "        [ 0.1440, -0.4446,  0.7735, -0.3308, -0.1402],\n",
       "        [ 0.2125, -0.4414,  0.8597, -0.2617, -0.1880],\n",
       "        [ 0.0673, -0.4824,  0.6494, -0.3697, -0.1208],\n",
       "        [ 0.1209, -0.4426,  0.7435, -0.3585, -0.1300],\n",
       "        [ 0.0573, -0.6288,  0.5925, -0.1287, -0.1710],\n",
       "        [ 0.1251, -0.4189,  0.7605, -0.3810, -0.1277]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "depth = 4\n",
    "width = 16\n",
    "activation = FinalActivation()\n",
    "model = RMHDPINN(input_size=2, output_size=5, n_hidden=depth, hidden_width=width, activation=TrainableTanh(), output_projection=activation).to(device)\n",
    "print(model)\n",
    "model(torch.rand(10,device=device), torch.rand(10,device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates the initial conditions for primitives from conserved quantities.\n",
    "\n",
    "Parameters:\n",
    "- x (torch.Tensor): The spatial coordinate.\n",
    "- Gamma (float): The adiabatic index.\n",
    "- v_z (float): The z-component of the velocity.\n",
    "- B_x (float): The x-component of the magnetic field.\n",
    "- B_z (float): The z-component of the magnetic field.\n",
    "\n",
    "Returns:\n",
    "- torch.Tensor: The initial conditions for primitives.\n",
    "\n",
    "\"\"\"\n",
    "def initial_condition(x):\n",
    "    # Conserved: rho, v_x, v_y, By, p\n",
    "    # Primitives: rho, mx, my, By, E\n",
    "    condition1 = torch.tensor([1.0, 0.0, 0.0, 6.0, 30.0], device=device)\n",
    "    condition2 = torch.tensor([1.0, 0.0, 0.0, 0.7, 1.0], device=device)\n",
    "    newx = torch.flatten(x).to(device)\n",
    "    return torch.outer(newx<0, condition1) + torch.outer(newx>=0, condition2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(outputs, inputs, order = 1, device=device):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]\n",
    "    elif order > 1:\n",
    "        return gradients(gradients(outputs, inputs, 1), inputs, order - 1) # Recursively take gradients\n",
    "    else:\n",
    "        return outputs\n",
    "\n",
    "def rmhd_residual(pred,t,x):\n",
    "    u_t= gradients(conserved_alfredo(pred),t)\n",
    "    u_x= gradients(currents_alfredo(pred),x)\n",
    "    return u_t + u_x\n",
    "\n",
    "\n",
    "def random_domain(num_samples, t_range, x_range):\n",
    "    t_random = torch.zeros(size=(num_samples, 1), device=device).uniform_(*t_range)\n",
    "    x_random = torch.zeros(size=(num_samples, 1), device=device).uniform_(*x_range)\n",
    "    t_random.requires_grad = True\n",
    "    x_random.requires_grad = True\n",
    "    return t_random, x_random\n",
    "\n",
    "\n",
    "\n",
    "def random_boundary(num_samples, t_range, x_range, initial_to_boundary_ratio = 0.5):\n",
    "    num_initial = int(initial_to_boundary_ratio * num_samples)\n",
    "    num_boundary = num_samples - num_initial\n",
    "    t_min, t_max = t_range\n",
    "\n",
    "    # Generate initial condition samples\n",
    "    t_initial = torch.zeros(size=(num_initial, 1), device=device)\n",
    "    x_initial = torch.zeros(size=(num_initial, 1), device=device).uniform_(*x_range)\n",
    "    u_initial = initial_condition(x_initial)\n",
    "\n",
    "    # Generate boundary condition samples\n",
    "    t_boundary = torch.zeros(size=(num_boundary, 1), device=device).uniform_(*t_range)\n",
    "\n",
    "    # We assume x_range = (-1, 1) here\n",
    "    x_boundary = 2 * torch.randint(0, 2, size=(num_boundary, 1), device=device) - 1\n",
    "    u_boundary = initial_condition(x_boundary)#np.zeros((num_boundary, 1))\n",
    "\n",
    "    return torch.tensor(t_initial, dtype=torch.float32), torch.tensor(x_initial, dtype=torch.float32), torch.tensor(u_initial, dtype=torch.float32), \\\n",
    "           torch.tensor(t_boundary, dtype=torch.float32), torch.tensor(x_boundary, dtype=torch.float32), torch.tensor(u_boundary, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "model = RMHDPINN(input_size=2, output_size=5, n_hidden=depth, hidden_width=width, activation=TrainableTanh(), output_projection=activation).to(device)\n",
    "#model = PINN(input_size=2, output_size=5, n_hidden=depth, hidden_width=width, activation=activation)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, eps=1e-4)\n",
    "boundary_criterion = nn.MSELoss()\n",
    "initial_criterion = nn.MSELoss()\n",
    "domain_criterion = nn.MSELoss()\n",
    "intermediate_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/qsgfh7hj7csg_x4f7kcm2p9r0000gq/T/ipykernel_3311/2172649982.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(t_initial, dtype=torch.float32), torch.tensor(x_initial, dtype=torch.float32), torch.tensor(u_initial, dtype=torch.float32), \\\n",
      "/var/folders/69/qsgfh7hj7csg_x4f7kcm2p9r0000gq/T/ipykernel_3311/2172649982.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(t_boundary, dtype=torch.float32), torch.tensor(x_boundary, dtype=torch.float32), torch.tensor(u_boundary, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 141.21878051757812\n",
      "Epoch: 200, Loss: 126.5076904296875\n",
      "Epoch: 300, Loss: 55.898582458496094\n",
      "Epoch: 400, Loss: 30.223264694213867\n",
      "Epoch: 500, Loss: 21.002304077148438\n",
      "Epoch: 600, Loss: 11.69563102722168\n",
      "Epoch: 700, Loss: 13.825124740600586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mdomain_loss \u001b[38;5;241m+\u001b[39m lambda_initial \u001b[38;5;241m*\u001b[39m initial_loss \u001b[38;5;241m+\u001b[39m lambda_boundary \u001b[38;5;241m*\u001b[39m boundary_loss\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Compute backward and update model parameters as usual\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m current_grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n",
      "File \u001b[0;32m~/Software/SMART-GRMHD/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Software/SMART-GRMHD/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 3000\n",
    "model.train()\n",
    "\n",
    "x_range, t_range = [-0.5, 0.5], [0, 1]\n",
    "loss_history = []\n",
    "domain_loss_history = []\n",
    "initial_loss_history = []\n",
    "boundary_loss_history = []\n",
    "a_history = []\n",
    "b_history = []\n",
    "c_history = []\n",
    "gradient_history = []\n",
    "\n",
    "# Initialize lambda values\n",
    "lambda_initial = 1.0\n",
    "lambda_boundary = 1.0\n",
    "\n",
    "max_schedule_steps = 1000\n",
    "n = 100\n",
    "for epoch in range(1, num_epochs + 1, max_schedule_steps):\n",
    "    n += 7\n",
    "    domain_samples = n**2\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] *= 3/4\n",
    "    for step in range(0, max_schedule_steps):\n",
    "        boundary_samples = 300  # You can change this value\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        domain_t, domain_x = random_domain(domain_samples, t_range, x_range)\n",
    "        initial_t, initial_x, initial_u, boundary_t, boundary_x, boundary_u = random_boundary(boundary_samples, t_range, x_range, 0.5)\n",
    "        domain_prediction = model(domain_t, domain_x)\n",
    "        domain_residual = rmhd_residual(domain_prediction, domain_t, domain_x)\n",
    "        initial_prediction = model(initial_t, initial_x)\n",
    "        boundary_prediction = model(boundary_t, boundary_x)\n",
    "\n",
    "        domain_loss = domain_criterion(domain_residual, torch.zeros_like(domain_residual))\n",
    "        initial_loss = initial_criterion(initial_prediction, initial_u)\n",
    "        boundary_loss = boundary_criterion(boundary_prediction, boundary_u)\n",
    "\n",
    "        # # Step 3: Compute total loss and backpropagate\n",
    "        loss = 10*domain_loss + lambda_initial * initial_loss + lambda_boundary * boundary_loss\n",
    "\n",
    "        # Compute backward and update model parameters as usual\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_grads = [p.grad.clone() for p in model.parameters()]\n",
    "        gradient_history.append(current_grads)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        domain_loss_history.append(domain_loss.item())\n",
    "        initial_loss_history.append(initial_loss.item())\n",
    "        boundary_loss_history.append(boundary_loss.item())\n",
    "\n",
    "        \n",
    "        if (epoch+step) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+step}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(domain_t, domain_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tplot = 0  # 0.147\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, 100)\n",
    "t = np.linspace(tplot, tplot, 1)\n",
    "\n",
    "t, x = np.meshgrid(t, x)\n",
    "t, x = t.flatten(), x.flatten()\n",
    "t, x = torch.Tensor(t).to(device), torch.Tensor(x).to(device)\n",
    "# numeric = inter_condition(tplot, x)\n",
    "# numeric = numeric.reshape(100, 7)\n",
    "prediction = model(t, x).detach()\n",
    "prediction = prediction.reshape(100, 5)\n",
    "\n",
    "# plt.imshow(prediction)\n",
    "\n",
    "for i in range(7):\n",
    "\n",
    "    # plt.plot(x.cpu().numpy(), numeric[:, i].cpu().flatten().numpy())\n",
    "    plt.plot(x.cpu().numpy(), prediction[:, i].cpu().flatten().numpy())\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartGRMHD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
